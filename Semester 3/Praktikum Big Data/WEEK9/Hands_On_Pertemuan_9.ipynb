{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "115f65d3",
   "metadata": {},
   "source": [
    "# Hands-On Pertemuan 9: Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0fa7f9",
   "metadata": {},
   "source": [
    "## Tujuan:\n",
    "- Mengasah keterampilan analisis data menggunakan Spark SQL.\n",
    "- Melakukan lebih banyak latihan SQL yang mengarah ke skenario dunia nyata.\n",
    "- Mempersiapkan mahasiswa untuk menggunakan Spark SQL dalam proyek besar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a45dcb9",
   "metadata": {},
   "source": [
    "### 1. Refresher: Basic SQL Operations in Spark SQL\n",
    "- **Tugas 1**: Ulangi pemahaman Anda tentang SQL dasar dengan menjalankan queries sederhana pada dataset di Spark SQL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "974a023b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+------+------+------+\n",
      "| Name|Age|Gender|Salary|DeptId|\n",
      "+-----+---+------+------+------+\n",
      "|James| 34|     M|  3000|     1|\n",
      "| Anna| 28|     F|  4100|     2|\n",
      "|  Lee| 23|     M|  2700|     1|\n",
      "+-----+---+------+------+------+\n",
      "\n",
      "+----+---+\n",
      "|Name|Age|\n",
      "+----+---+\n",
      "|Anna| 28|\n",
      "+----+---+\n",
      "\n",
      "+------------------+\n",
      "|       avg(Salary)|\n",
      "+------------------+\n",
      "|3266.6666666666665|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Pertemuan9').getOrCreate()\n",
    "\n",
    "data = [\n",
    "    ('James', 34, 'M', 3000, 1),\n",
    "    ('Anna', 28, 'F', 4100, 2),\n",
    "    ('Lee', 23, 'M', 2700, 1)\n",
    "]\n",
    "columns = ['Name', 'Age', 'Gender', 'Salary', 'DeptId']\n",
    "df = spark.createDataFrame(data, schema=columns)\n",
    "df.createOrReplaceTempView('employees')\n",
    "spark.sql('SELECT * FROM employees').show()\n",
    "spark.sql('SELECT Name, Age FROM employees WHERE Salary > 3000').show()\n",
    "spark.sql('SELECT AVG(Salary) FROM employees').show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd8b4f1",
   "metadata": {},
   "source": [
    "### 2. Advanced Queries for Data Analysis\n",
    "Gunakan queries lebih kompleks, melibatkan grouping, filtering, dan subqueries.\n",
    "- **Tugas 2**: Buat SQL query yang menghitung total gaji berdasarkan jenis kelamin dan usia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fc8a9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+---+\n",
      "|Gender|TotalSalary|Age|\n",
      "+------+-----------+---+\n",
      "|     M|       2700| 23|\n",
      "|     F|       4100| 28|\n",
      "|     M|       3000| 34|\n",
      "+------+-----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "SELECT Gender, SUM(Salary) as TotalSalary, Age\n",
    "FROM employees\n",
    "GROUP BY Gender, Age\n",
    "ORDER BY Age\n",
    "''').show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b3ccb1",
   "metadata": {},
   "source": [
    "- **Tugas Tambahan 2**: \n",
    "1. Cari rata-rata gaji per departemen.\n",
    "2. Temukan karyawan yang memiliki gaji di atas rata-rata untuk gender masing-masing.\n",
    "3. Buat ranking karyawan berdasarkan gaji dalam departemen mereka.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4fffdc",
   "metadata": {},
   "source": [
    "### 3. Penggunaan Window Functions dan Subqueries\n",
    "Latihan penggunaan window functions untuk menemukan karyawan dengan gaji tertinggi dan urutannya berdasarkan kelompok usia.\n",
    "- **Tugas 3**: Terapkan window functions untuk menemukan top 3 karyawan dalam kelompok usia tertentu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb927310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+------+----+\n",
      "| Name|Age|Salary|rank|\n",
      "+-----+---+------+----+\n",
      "|  Lee| 23|  2700|   1|\n",
      "| Anna| 28|  4100|   1|\n",
      "|James| 34|  3000|   1|\n",
      "+-----+---+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "SELECT Name, Age, Salary, ROW_NUMBER() OVER (PARTITION BY Age ORDER BY Salary DESC) as rank\n",
    "FROM employees\n",
    "''').show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb43ac4",
   "metadata": {},
   "source": [
    "### 4. Advanced Spark SQL Queries\n",
    "Menjelajahi queries yang lebih kompleks yang melibatkan multiple joins, subqueries, dan window functions.\n",
    "- **Tugas 4**: Demonstrasi penggunaan multi-level joins dan subqueries untuk analisis data yang mendalam.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1bfd9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 26:=================================================>      (14 + 2) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---------+-----------+\n",
      "| Name|Age| DeptName|ProjectName|\n",
      "+-----+---+---------+-----------+\n",
      "|  Lee| 23|       HR|  Project A|\n",
      "|James| 34|       HR|  Project A|\n",
      "| Anna| 28|Marketing|  Project B|\n",
      "+-----+---+---------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Pertemuan9').getOrCreate()\n",
    "\n",
    "# Data setup for complex SQL queries\n",
    "employees = spark.createDataFrame([\n",
    "    ('James', 34, 'M', 3000, 1),\n",
    "    ('Anna', 28, 'F', 4100, 2),\n",
    "    ('Lee', 23, 'M', 2700, 1)\n",
    "], ['Name', 'Age', 'Gender', 'Salary', 'DeptId'])\n",
    "departments = spark.createDataFrame([\n",
    "    (1, 'HR'),\n",
    "    (2, 'Marketing')\n",
    "], ['DeptId', 'DeptName'])\n",
    "projects = spark.createDataFrame([\n",
    "    (1, 'Project A'),\n",
    "    (2, 'Project B')\n",
    "], ['DeptId', 'ProjectName'])\n",
    "employees.createOrReplaceTempView('employees')\n",
    "departments.createOrReplaceTempView('departments')\n",
    "projects.createOrReplaceTempView('projects')\n",
    "\n",
    "# Complex SQL query involving multiple joins and subqueries\n",
    "spark.sql('''\n",
    "SELECT e.Name, e.Age, d.DeptName, p.ProjectName\n",
    "FROM employees e\n",
    "JOIN departments d ON e.DeptId = d.DeptId\n",
    "JOIN projects p ON e.DeptId = p.DeptId\n",
    "''').show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad25f1f0",
   "metadata": {},
   "source": [
    "Latihan mandiri untuk memperkuat pemahaman tentang Spark SQL dalam analisis data terdistribusi.\n",
    "- **Tugas 5**: Tuliskan query SQL untuk menemukan rata-rata gaji per departemen dan rangking setiap karyawan dalam departemen berdasarkan gaji.\n",
    "- **Tugas 6**: Gunakan window functions untuk menentukan tren gaji berdasarkan usia di setiap departemen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a2c206",
   "metadata": {},
   "source": [
    "### 5. Advanced Data Analysis and Visualization\n",
    "Penerapan teknik analisis data yang lebih canggih dan visualisasi menggunakan PySpark dan matplotlib.\n",
    "- **Tugas 7**: Lakukan analisis tren gaji menggunakan Spark SQL dan visualisasikan hasilnya.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a170f256",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'distutils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 10\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Advanced data analysis with visualization\u001b[39;00m\n\u001b[1;32m      5\u001b[0m salary_trends \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'''\u001b[39;49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;43mSELECT Age, AVG(Salary) AS AverageSalary\u001b[39;49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;43mFROM employees\u001b[39;49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;43mGROUP BY Age\u001b[39;49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;43mORDER BY Age\u001b[39;49m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;124;43m'''\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Visualization of salary trends\u001b[39;00m\n\u001b[1;32m     13\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n",
      "File \u001b[0;32m~/Documents/bigdata/spark/python/pyspark/sql/pandas/conversion.py:86\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _create_converter_to_pandas\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m require_minimum_pandas_version\n\u001b[0;32m---> 86\u001b[0m \u001b[43mrequire_minimum_pandas_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m     90\u001b[0m jconf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession\u001b[38;5;241m.\u001b[39m_jconf\n",
      "File \u001b[0;32m~/Documents/bigdata/spark/python/pyspark/sql/pandas/utils.py:24\u001b[0m, in \u001b[0;36mrequire_minimum_pandas_version\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# TODO(HyukjinKwon): Relocate and deduplicate the version specification.\u001b[39;00m\n\u001b[1;32m     22\u001b[0m minimum_pandas_version \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.0.5\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdistutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LooseVersion\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'distutils'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Advanced data analysis with visualization\n",
    "salary_trends = spark.sql('''\n",
    "SELECT Age, AVG(Salary) AS AverageSalary\n",
    "FROM employees\n",
    "GROUP BY Age\n",
    "ORDER BY Age\n",
    "''').toPandas()\n",
    "\n",
    "# Visualization of salary trends\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(salary_trends['Age'], salary_trends['AverageSalary'], marker='o')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Average Salary')\n",
    "plt.title('Salary Trends by Age')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd314ae9",
   "metadata": {},
   "source": [
    "### 6. Homework\n",
    "- **Tugas 1**: Gunakan Spark SQL untuk mencari total gaji dan jumlah karyawan per departemen. Buat visualisasi perbandingan antar departemen.\n",
    "- **Tugas 2**: Temukan karyawan dengan gaji di atas rata-rata dalam setiap kelompok usia dan visualisasikan data ini dalam bentuk grafik batang atau pie chart.\n",
    "- **Tugas 3**: Buat dataset yang lebih besar (misalnya, 100+ baris) dan lakukan analisis mendalam menggunakan SQL functions seperti `SUM()`, `AVG()`, `COUNT()`, serta `JOIN` antar tabel serta buat visualisasi yang menarik.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
